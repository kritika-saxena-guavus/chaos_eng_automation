{
	"version": "1.0.0",
	"title": "Experiment with bringing down the spark executor running node when the spark job '{{job_name}}' is running.",
	"description": "The spark should be able to handle executor failover and no data loss/corruption should be there.",
	"tags": ["spark", "application"],
	"controls": [{
		"name": "spark-related-controls",
		"provider": {
			"type": "python",
			"module": "chaostoolkit_nimble.controllers.application.spark.control"
		}
	}],
	"steady-state-hypothesis": {
		"title": "Job {{job_name}} is up and running on yarn.",
		"probes": [{
				"type": "probe",
				"name": "Check all ambari services are up on the cluster.   ",
				"tolerance": true,
				"provider": {
					"module": "chaostoolkit_nimble.core.utils.ambari_ha_utils",
					"type": "python",
					"func": "are_all_services_running"
				}
			},
			{
			"type": "probe",
			"name": "Check job {{job_name}} running on yarn.  ",
			"tolerance": true,
			"provider": {
				"module": "chaostoolkit_nimble.core.utils.yarn_ha_utils",
				"type": "python",
				"func": "is_job_running_on_yarn",
				"arguments": {
					"job_name": "{{job_name}}"
				}
			}
		}]
	},
	"method": [{
		"type": "action",
		"name": "Reboot node where spark job {{job_name}} executors are running.    ",
		"provider": {
			"module": "chaostoolkit_nimble.core.utils.node_ha_utils",
			"type": "python",
			"func": "reboot_spark_executor_node",
			"arguments": {
				"job_name": "{{job_name}}"
			}
		}
	}],
	"rollbacks": []
}